{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1HpoHNUbDGSVy4Y0TWcCTQF8p7_nIjmbm","authorship_tag":"ABX9TyOUW9IpBLHsEB249DxvZxMQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rDZt_2b3anQH"},"source":["# BERTでクラス分類をしてみよう"]},{"cell_type":"markdown","metadata":{"id":"Osww4ktHatfj"},"source":["## ライブラリのインストール・データのダウンロード"]},{"cell_type":"code","metadata":{"id":"KrAF3997hodk"},"source":["!pip install transformers[\"ja\"] nlp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"elu65_j8CjQN"},"source":["%cd \"/content/drive/My Drive/Colab Notebooks/introduction_to_pytorch_and_bert\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EYqx0dV7oLTj"},"source":["!mkdir data\n","!mkdir checkpoints\n","!wget https://github.com/tealgreen0503/introduction_to_pytorch_and_bert/raw/main/data/amazon_reviews_multilingual_JP_v1_00_20000_binary.tsv.gz -P data/\n","!wget https://github.com/tealgreen0503/introduction_to_pytorch_and_bert/raw/main/data/amazon_reviews_multilingual_JP_v1_00_10000_binary.tsv.gz -P data/\n","!gunzip -d ./data/amazon_reviews_multilingual_JP_v1_00_20000_binary.tsv.gz\n","!gunzip -d ./data/amazon_reviews_multilingual_JP_v1_00_10000_binary.tsv.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bBfn1-5ma0Yl"},"source":["## ライブラリのインポート・シードの固定・定数の設定"]},{"cell_type":"code","metadata":{"id":"NYKsV_GvhNih"},"source":["import os\n","import random\n","import collections\n","\n","from bs4 import BeautifulSoup\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from sklearn.metrics import f1_score, classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from tqdm.notebook import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from transformers import AutoTokenizer, AutoModel, AdamW\n","import nlp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMaywe9tZxr8"},"source":["SEED = 42\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","        \n","seed_everything(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YvUQ3RD8gfue"},"source":["if torch.cuda.is_available():\n","    current_device = torch.cuda.current_device()\n","    print('Device:', torch.cuda.get_device_name(current_device))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zWSIxmYXiCPy"},"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","TRAIN_PATH = './data/amazon_reviews_multilingual_JP_v1_00_20000_binary.tsv'\n","TEST_PATH = './data/amazon_reviews_multilingual_JP_v1_00_10000_binary.tsv'\n","CKPT_DIR = './checkpoints/'\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n","TRAIN_BATCH_SIZE = 32\n","VALID_BATCH_SIZE = 128\n","NUM_CLASSES = 2\n","NUM_EPOCH = 5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x4gH7LN3a-sd"},"source":["## データの確認"]},{"cell_type":"code","metadata":{"id":"Wc-6s2bOr_4m"},"source":["train_df = pd.read_csv(TRAIN_PATH, sep='\\t')\n","train_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sUt-PB6Mzp01"},"source":["train_df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvfz_zly010C"},"source":["train_df['binary_star_rating'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QLMhlQMx098M"},"source":["def load_data(data_path, target='binary_star_rating'):\n","    df = pd.read_csv(data_path, sep='\\t')\n","    df = df[['review_body', target]]\n","\n","    def clean_html(html, strip=True):\n","        soup = BeautifulSoup(html, 'html.parser')\n","        text = soup.get_text(strip=strip)\n","        return text\n","\n","    df['review_body'] = df['review_body'].map(clean_html)\n","    df = df.rename(columns={target: 'labels'})\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FiAlO8kxbC8v"},"source":["df = load_data(TRAIN_PATH)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RqsQz-1ubRod"},"source":["## 関数・モデルの定義"]},{"cell_type":"code","metadata":{"id":"0TbY7nylJjJu"},"source":["def make_dataset(df, tokenizer, device):\n","    dataset = nlp.Dataset.from_pandas(df)\n","    dataset = dataset.map(\n","        lambda example: tokenizer(example[\"review_body\"],\n","                                  padding=\"max_length\",\n","                                  truncation=True,\n","                                  max_length=128))\n","    dataset.set_format(type='torch', \n","                       columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'], \n","                       device=device)\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kV2Ni6hUVi8"},"source":["class Classifier(nn.Module):\n","    def __init__(self, model_name, num_classes=2):\n","        super().__init__()\n","\n","        self.bert = AutoModel.from_pretrained(model_name)\n","        self.dropout = nn.Dropout(0.1)\n","        self.linear = nn.Linear(768, num_classes)\n","        nn.init.normal_(self.linear.weight, std=self.bert.config.initializer_range)\n","        nn.init.zeros_(self.linear.bias)\n","\n","    def forward(self, **inputs):\n","        outputs = self.bert(**inputs)\n","        output = outputs.last_hidden_state\n","        output = output[:, 0, :]\n","        output = self.dropout(output)\n","        output = self.linear(output)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4C1XwhLpYMRU"},"source":["class Trainer:\n","    def __init__(self,\n","                 model,\n","                 train_dataloader,\n","                 valid_dataloader,\n","                 criterion,\n","                 optimizer,\n","                 scheduler=None,\n","                 num_epoch=10,\n","                 ckpt_name='./bert'):\n","        \n","        self.model = model\n","        self.train_dataloader = train_dataloader\n","        self.valid_dataloader = valid_dataloader\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        if scheduler is not None:\n","            self.scheduler = scheduler\n","        else:\n","            self.scheduler = optim.lr_scheduler.StepLR(self.optimizer,\n","                                                       step_size=1e+10,\n","                                                       gamma=1.0)\n","        self.num_epoch = num_epoch\n","        self.ckpt_name = ckpt_name\n","\n","\n","    def _train_step(self, epoch):\n","        self.model.train()\n","        total_loss = 0\n","        total_corrects = 0\n","        all_labels = np.array([])\n","        all_preds = np.array([])\n","\n","        progress = tqdm(self.train_dataloader, total=len(self.train_dataloader))\n","\n","        for i, batch in enumerate(progress):\n","            progress.set_description(f\"<Train> Epoch{epoch+1}\")\n","\n","            labels = batch.pop('labels')\n","            inputs = batch\n","\n","            self.optimizer.zero_grad()\n","\n","            output = self.model(**inputs)\n","            loss = self.criterion(output, labels)\n","            preds = torch.argmax(output, dim=1)\n","\n","            loss.backward()\n","            self.optimizer.step()\n","            self.scheduler.step()\n","\n","            total_loss += loss.item()\n","            total_corrects += torch.sum(preds == labels)\n","            all_labels = np.r_[all_labels, labels.to('cpu').detach().numpy()]\n","            all_preds = np.r_[all_preds, preds.to('cpu').detach().numpy()]\n","            f1 = f1_score(all_labels, all_preds)\n","\n","            progress.set_postfix(loss=total_loss/(i+1), f1=f1)\n","\n","        train_loss = total_loss / len(self.train_dataloader)\n","        train_acc = total_corrects.to('cpu').detach().numpy() / len(self.train_dataloader.dataset)\n","        train_f1 = f1\n","\n","        return train_loss, train_acc, train_f1\n","\n","    def _eval_step(self, epoch):\n","        self.model.eval()\n","        total_loss = 0\n","        total_corrects = 0\n","        all_labels = np.array([])\n","        all_preds = np.array([])\n","\n","        with torch.no_grad():\n","            progress = tqdm(self.valid_dataloader,\n","                            total=len(self.valid_dataloader))\n","            \n","            for i, batch in enumerate(progress):\n","                progress.set_description(f\"<Valid> Epoch{epoch+1}\")\n","\n","                labels = batch.pop('labels')\n","                inputs = batch\n","\n","                output = self.model(**inputs)\n","                loss = self.criterion(output, labels)\n","                preds = torch.argmax(output, dim=1)\n","                \n","                total_loss += loss.item()\n","                total_corrects += torch.sum(preds == labels)\n","                all_labels = np.r_[all_labels, labels.to('cpu').detach().numpy()]\n","                all_preds = np.r_[all_preds, preds.to('cpu').detach().numpy()]\n","                f1 = f1_score(all_labels, all_preds)\n","\n","                progress.set_postfix(loss=total_loss/(i+1), f1=f1)\n","\n","            valid_loss = total_loss / len(self.valid_dataloader)\n","            valid_acc = total_corrects.to('cpu').detach().numpy() / len(self.valid_dataloader.dataset)\n","            valid_f1 = f1\n","\n","        return valid_loss, valid_acc, valid_f1\n","\n","    def train(self, metric='f1'):\n","        if metric == 'f1':\n","            best_metric = 0\n","        elif metric == 'acc':\n","            best_metric = 0\n","        elif metric == 'loss':\n","            best_metric = np.inf\n","        else:\n","            raise RuntimeError()\n","\n","        for epoch in range(self.num_epoch):\n","            train_loss, train_acc, train_f1= self._train_step(epoch)\n","            valid_loss, valid_acc, valid_f1 = self._eval_step(epoch)\n","            print(f'Loss: {valid_loss}  Acc: {valid_acc}  f1: {valid_f1}', end='  ')\n","\n","            if metric == 'f1':\n","                if valid_f1 > best_metric:\n","                    best_metric = valid_f1\n","                    print('model saving!', end='')\n","                    torch.save(self.model.state_dict(), f\"{self.ckpt_name}.pth\")\n","            elif metric == 'acc':\n","                if valid_acc > best_metric:\n","                    best_metric = valid_acc\n","                    print('model saving!', end='')\n","                    torch.save(self.model.state_dict(), f\"{self.ckpt_name}.pth\")\n","            elif metric == 'loss':\n","                if valid_loss < best_metric:\n","                    best_metric = valid_loss\n","                    print('model saving!', end='')\n","                    torch.save(self.model.state_dict(), f\"{self.ckpt_name}.pth\")\n","            else:\n","                raise RuntimeError()\n","            print('\\n\\n')\n","\n","        return best_metric"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mvFoBy6ibXwT"},"source":["## 学習"]},{"cell_type":"code","metadata":{"id":"tQa1KR5c4Ia8"},"source":["df = load_data(TRAIN_PATH)\n","train_df, valid_df = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df['labels'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oM46SQSIOr4n"},"source":["tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","train_dataset = make_dataset(train_df, tokenizer, DEVICE)\n","valid_dataset = make_dataset(valid_df, tokenizer, DEVICE)\n","\n","train_dataloader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True\n",")\n","valid_dataloader = torch.utils.data.DataLoader(\n","    valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gGmhiowMg3-s"},"source":["model = Classifier(MODEL_NAME, num_classes=NUM_CLASSES)\n","model = model.to(DEVICE)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","ckpt_name = CKPT_DIR + MODEL_NAME.replace('/', '_')\n","\n","trainer = Trainer(model,\n","                  train_dataloader,\n","                  valid_dataloader,\n","                  criterion,\n","                  optimizer,\n","                  num_epoch=NUM_EPOCH,\n","                  ckpt_name=ckpt_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"it5L7zthiCRG"},"source":["valid_f1 = trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vaw_KeSHhifo"},"source":["## テスト"]},{"cell_type":"code","metadata":{"id":"CZYKIkVHW3Zk"},"source":["model = Classifier(MODEL_NAME, num_classes=NUM_CLASSES)\n","model.load_state_dict(torch.load(ckpt_name + '.pth'))\n","model.to(DEVICE)\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-tGWuCnRWKMX"},"source":["test_df = load_data(TEST_PATH)\n","test_dataset = make_dataset(test_df, tokenizer, DEVICE)\n","test_dataloader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDW_m91iYczQ"},"source":["with torch.no_grad():\n","    progress = tqdm(test_dataloader, total=len(test_dataloader))\n","    final_output = np.array([])\n","\n","    for batch in progress:\n","        progress.set_description(\"<Test>\")\n","\n","        _ = batch.pop('labels')\n","        inputs = batch\n","\n","        output = model(**inputs)\n","        output = torch.softmax(output, dim=1).to('cpu').detach().numpy()\n","        output = np.argmax(output, axis=1)\n","\n","        final_output = np.r_[final_output, output]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wfevql7LZth_"},"source":["test_acc = np.sum(test_df['labels'] == final_output) / len(test_df['labels'])\n","test_f1 = f1_score(test_df['labels'], final_output)\n","\n","print(f'Test Acc: {test_acc}  Test f1: {test_f1}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XD1tixx_XDWm"},"source":["def print_classification_report(all_labels, all_preds):\n","    cr = classification_report(all_labels, all_preds)\n","    print(cr)\n","    freq = collections.Counter(all_labels)\n","    freq = [freq[i] for i in range(NUM_CLASSES)]\n","    cm = confusion_matrix(all_labels, all_preds)\n","    cm = cm / freq\n","    sns.heatmap(cm, cmap=\"Reds\", annot=True)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eIeNBeLbXTfP"},"source":["print_classification_report(test_df['labels'], final_output)"],"execution_count":null,"outputs":[]}]}